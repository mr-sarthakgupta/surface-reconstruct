{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from skimage import measure\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "# import your libraries\n",
    "\n",
    "from IGR.code.model.network import ImplicitNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[Open3D WARNING] Read PLY failed: unable to open file: bunny.ply\u001b[0;m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPly: Unable to open file\n"
     ]
    }
   ],
   "source": [
    "# load point cloud\n",
    "points = load_pointcloud('bunny.ply')\n",
    "\n",
    "# instantiate the model and optimizer\n",
    "model = ImplicitNet(d_in = 3, dims = [ 512, 512, 512, 512, 512, 512, 512, 512 ], skip_in = [4], geometric_init = True)\n",
    "opt = torch.optim.Adam(\n",
    "            [\n",
    "                {\n",
    "                    \"params\": model.parameters(),\n",
    "                    \"lr\": 0.005,\n",
    "                    \"weight_decay\": 0\n",
    "                },\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pointcloud(filename):\n",
    "    pcd = o3d.io.read_point_cloud(filename)\n",
    "    points = torch.tensor(np.asarray(pcd.points), dtype=torch.float32)\n",
    "    return points\n",
    "    \n",
    "def write_mesh(v,f,filename):\n",
    "    mesh = o3d.geometry.TriangleMesh(o3d.utility.Vector3dVector(v),o3d.utility.Vector3iVector(f))\n",
    "    o3d.io.write_triangle_mesh(filename,mesh)\n",
    "    \n",
    "def write_pointcloud(p,filename):\n",
    "    pc = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(p))\n",
    "    o3d.io.write_point_cloud(filename,pc)\n",
    "\n",
    "# class ImplcitNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         pass\n",
    "\n",
    "#     def phase_loss(self, x):\n",
    "#         pass\n",
    "\n",
    "class PHASELoss(nn.Module):\n",
    "    def __init__(self, epsilon=0.01, lambda_val=0.1, mu=0.1, ball_radius=0.01, iter_points = 50, use_normals=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            epsilon: Regularization parameter that controls smoothness\n",
    "            lambda_val: Weight for the reconstruction loss\n",
    "            mu: Weight for the normal/gradient constraint loss\n",
    "            ball_radius: Radius of balls around point samples for reconstruction loss\n",
    "            use_normals: If True, uses provided normals; otherwise enforces unit gradients\n",
    "        \"\"\"\n",
    "        super(PHASELoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.lambda_val = lambda_val\n",
    "        self.mu = mu\n",
    "        self.ball_radius = ball_radius\n",
    "        self.use_normals = use_normals\n",
    "        \n",
    "    def double_well_potential(self, x):\n",
    "        return x**2 - 2*torch.abs(x) + 1\n",
    "    \n",
    "    def reconstruction_loss(self, u, points, sample_count=55):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            u: Neural network representing the signed density\n",
    "            points: Input point cloud (B x N x 3)\n",
    "            sample_count: Number of points to sample in each ball\n",
    "        \"\"\"\n",
    "        \n",
    "        og_points = points.clone()\n",
    "\n",
    "        random_offsets = torch.rand((sample_count, og_points.shape[-1]))\n",
    "        random_offsets = random_offsets / torch.norm(random_offsets, dim=-1, keepdim=True)\n",
    "        # random_offsets = random_offsets * self.ball_radius * torch.rand_like(random_offsets[..., :1])\n",
    "        random_offsets = random_offsets * self.ball_radius \n",
    "\n",
    "        # Sample points within balls\n",
    "        points = og_points.unsqueeze(1).expand(-1, sample_count, -1) + random_offsets.unsqueeze(0).expand(og_points.shape[0], -1, -1)\n",
    "        \n",
    "        u_values = []\n",
    "        for point in points:\n",
    "            u_values.append(u(points).mean().abs())\n",
    "\n",
    "        return torch.stack(u_values, dim = 0).mean()\n",
    "\n",
    "    def w(self, epsilon, u, x):\n",
    "        return -torch.sqrt(self.epsilon) * torch.log(1 - torch.abs(u(x))) * torch.sign(u(x))      \n",
    "\n",
    "    def normal_loss(self, w, points):\n",
    "        w_outs = w(points)\n",
    "        grad_outputs = torch.ones_like(w_outs)\n",
    "        w_grads = torch.autograd.grad(\n",
    "            outputs=w_outs,\n",
    "            inputs=points,\n",
    "            grad_outputs=grad_outputs,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )\n",
    "\n",
    "        return torch.mean(torch.norm(1 - torch.norm(w_grad, p = 2, dim = -1), p = 1, dim = -1) ** 2)\n",
    "\n",
    "    \n",
    "    def gradient_loss(self, u, points, dim_range):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            u: Neural network representing the signed density\n",
    "            w: Log-transformed function (-sqrt(epsilon) * log(1-|u|) * sign(u))\n",
    "            range: Input point cloud\n",
    "            normals: Surface normals (optional)\n",
    "        \"\"\"\n",
    "        inte = torch.zeros((points.shape[0]))\n",
    "\n",
    "        # Create 3D grid from the provided range\n",
    "        x_min, x_max = dim_range[0], dim_range[1]\n",
    "        y_min, y_max = dim_range[0], dim_range[1]\n",
    "        z_min, z_max = dim_range[0], dim_range[1]\n",
    "\n",
    "        steps = 10  # Number of steps in each dimension\n",
    "        x_vals = torch.linspace(x_min, x_max, steps)\n",
    "        y_vals = torch.linspace(y_min, y_max, steps)\n",
    "        z_vals = torch.linspace(z_min, z_max, steps)\n",
    "\n",
    "        # Volume element for integration\n",
    "        dx = (x_max - x_min) / steps\n",
    "        dy = (y_max - y_min) / steps\n",
    "        dz = (z_max - z_min) / steps\n",
    "        volume_element = dx * dy * dz\n",
    "\n",
    "        for i in range(steps):\n",
    "            for j in range(steps):\n",
    "                for k in range(steps):\n",
    "                    curr_points = torch.tensor([[x_vals[i], y_vals[j], z_vals[k]]], requires_grad=True)\n",
    "                    curr_points.requires_grad_(True)\n",
    "                    u_outs = u(curr_points)\n",
    "                    \n",
    "                    # Compute gradients of w with respect to input points\n",
    "                    grad_outputs = torch.ones_like(u_outs)\n",
    "                    gradients = torch.autograd.grad(\n",
    "                        outputs=u_outs,\n",
    "                        inputs=curr_points,\n",
    "                        grad_outputs=grad_outputs,\n",
    "                        create_graph=True,\n",
    "                        retain_graph=True,\n",
    "                        only_inputs=True\n",
    "                    )\n",
    "                    \n",
    "                    inte += torch.norm(gradients[0], dim=-1, p=2)**2 * volume_element      \n",
    "\n",
    "        print(inte.shape)\n",
    "\n",
    "        return inte \n",
    "        \n",
    "    def forward(self, model, points, dim_range, normals=None):\n",
    "        \"\"\"\n",
    "        Computes the complete PHASE loss\n",
    "        \n",
    "        Args:\n",
    "            model: Neural network model for signed density function\n",
    "            points: Input point cloud\n",
    "            normals: Surface normals (optional)\n",
    "        \"\"\"\n",
    "        \n",
    "        u = lambda x: model(x)\n",
    "\n",
    "        # Define the log-transformed function w (the smoothed SDF)\n",
    "        # w = lambda x: -torch.sqrt(self.epsilon) * torch.log(1 - torch.abs(u(x))) * torch.sign(u(x))      \n",
    "        \n",
    "        double_well_term = self.double_well_potential(u(points))\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        recon_loss = self.reconstruction_loss(u, points)\n",
    "        \n",
    "        # gradient loss\n",
    "        gradient_loss = self.gradient_loss(u, points, dim_range)\n",
    "\n",
    "        # normal loss\n",
    "        if normals is not None:\n",
    "            normal_loss = self.normal_loss(u, points, normals)\n",
    "        else:\n",
    "            normal_loss = None\n",
    "        \n",
    "        if normals is not None:\n",
    "            total_loss = self.epsilon * gradient_loss + double_well_term + self.lambda_val * recon_loss + self.mu * normal_loss\n",
    "        else:\n",
    "            total_loss = self.epsilon * gradient_loss + double_well_term + self.lambda_val * recon_loss\n",
    "        \n",
    "        return total_loss, {\n",
    "            'grad_term': gradient_loss,\n",
    "            'double_well': double_well_term,\n",
    "            'reconstruction': recon_loss,\n",
    "            'normal_constraint': normal_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chamfer_distance(pred_points, gt_points):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pred_points (torch.Tensor): Predicted point cloud (N x 3)\n",
    "        gt_points (torch.Tensor): Ground truth point cloud (M x 3)\n",
    "    \"\"\"\n",
    "    # Ensure inputs are on the same device\n",
    "    device = pred_points.device\n",
    "    \n",
    "    # Compute all pairwise distances\n",
    "    pred_expanded = pred_points.unsqueeze(1)  # (N, 1, 3)\n",
    "    gt_expanded = gt_points.unsqueeze(0)      # (1, M, 3)\n",
    "    \n",
    "    # Compute squared distances\n",
    "    dist_matrix = torch.sum((pred_expanded - gt_expanded) ** 2, dim=-1)  # (N, M)\n",
    "    \n",
    "    # Compute minimum distances in both directions\n",
    "    dist_pred_to_gt = torch.min(dist_matrix, dim=1)[0]  # (N,)\n",
    "    dist_gt_to_pred = torch.min(dist_matrix, dim=0)[0]  # (M,)\n",
    "    \n",
    "    # Average the distances (symmetric Chamfer distance)\n",
    "    chamfer_dist = torch.mean(dist_pred_to_gt) + torch.mean(dist_gt_to_pred)\n",
    "    \n",
    "    return chamfer_dist.item()\n",
    "\n",
    "def sample_mesh_points(mesh_path, n_points=10000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        mesh_path (str): Path to the mesh file\n",
    "        n_points (int): Number of points to sample    \n",
    "    \"\"\"\n",
    "    mesh = o3d.io.read_triangle_mesh(mesh_path)\n",
    "    pcd = mesh.sample_points_uniformly(number_of_points=n_points)\n",
    "    points = torch.tensor(np.asarray(pcd.points), dtype=torch.float32)\n",
    "    return points\n",
    "\n",
    "def evaluate_reconstruction(model, gt_mesh_path, resolution=64, bounds=(-1.0, 1.0), n_points=10000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model: Neural network model for implicit function\n",
    "        gt_mesh_path (str): Path to ground truth mesh\n",
    "        resolution (int): Resolution for marching cubes grid\n",
    "        bounds (tuple): Min and max bounds for the grid\n",
    "        n_points (int): Number of points to sample for Chamfer distance    \n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Create grid for marching cubes\n",
    "        x = np.linspace(bounds[0], bounds[1], resolution)\n",
    "        y = np.linspace(bounds[0], bounds[1], resolution)\n",
    "        z = np.linspace(bounds[0], bounds[1], resolution)\n",
    "        \n",
    "        X, Y, Z = np.meshgrid(x, y, z, indexing='ij')\n",
    "        points = torch.tensor(np.stack([X.flatten(), Y.flatten(), Z.flatten()], axis=1), \n",
    "                              dtype=torch.float32)\n",
    "        \n",
    "        # Process in batches to avoid memory issues\n",
    "        batch_size = 10000\n",
    "        sdf_grid = []\n",
    "        for i in range(0, points.shape[0], batch_size):\n",
    "            batch_points = points[i:i+batch_size]\n",
    "            sdf_batch = model(batch_points).detach().cpu().numpy()\n",
    "            sdf_grid.append(sdf_batch)\n",
    "        \n",
    "        sdf_grid = np.concatenate(sdf_grid, axis=0).reshape(resolution, resolution, resolution)\n",
    "        \n",
    "        # Generate mesh using marching cubes\n",
    "        v, f, _, _ = measure.marching_cubes_lewiner(sdf_grid, 0, gradient_direction='ascent')\n",
    "        \n",
    "        # Scale vertices back to original coordinate system\n",
    "        v = v / (resolution - 1) * (bounds[1] - bounds[0]) + bounds[0]\n",
    "        \n",
    "        # First save reconstructed mesh to temporary file\n",
    "        temp_mesh_path = 'temp_reconstruction.ply'\n",
    "        write_mesh(v, f, temp_mesh_path)\n",
    "        \n",
    "        # Sample points from both meshes\n",
    "        pred_points = sample_mesh_points(temp_mesh_path, n_points)\n",
    "        gt_points = sample_mesh_points(gt_mesh_path, n_points)\n",
    "        \n",
    "        # Compute Chamfer distance\n",
    "        chamfer_dist = compute_chamfer_distance(pred_points, gt_points)\n",
    "        \n",
    "    return chamfer_dist, v, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'normal_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m     loss, loss_components \u001b[38;5;241m=\u001b[39m loss_fn(model, selected_points, points_range, normals)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     loss, loss_components \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints_range\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     38\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[112], line 171\u001b[0m, in \u001b[0;36mPHASELoss.forward\u001b[0;34m(self, model, points, dim_range, normals)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m*\u001b[39m gradient_loss \u001b[38;5;241m+\u001b[39m double_well_term \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_val \u001b[38;5;241m*\u001b[39m recon_loss\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss, {\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad_term\u001b[39m\u001b[38;5;124m'\u001b[39m: gradient_loss,\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdouble_well\u001b[39m\u001b[38;5;124m'\u001b[39m: double_well_term,\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreconstruction\u001b[39m\u001b[38;5;124m'\u001b[39m: recon_loss,\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormal_constraint\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mnormal_loss\u001b[49m\n\u001b[1;32m    172\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normal_loss' is not defined"
     ]
    }
   ],
   "source": [
    "iters=100000\n",
    "lam, eps, mu = [10, 0.01, 10]\n",
    "loss_fn = PHASELoss(epsilon=eps, lambda_val=lam, mu=mu, ball_radius=0.001, use_normals=False)\n",
    "\n",
    "gt_mesh_path = \"Preimage_Implicit_DLTaskData/meshes/armadillo.obj\"\n",
    "\n",
    "normals = False\n",
    "\n",
    "if normals:\n",
    "    # Load normals if available\n",
    "    normals = load_pointcloud('bunny_normals.ply')\n",
    "else:    \n",
    "    normals = None\n",
    "\n",
    "model.train()\n",
    "\n",
    "gt_points_all = sample_mesh_points(gt_mesh_path, n_points=10000)\n",
    "\n",
    "points_range = (gt_points_all.min(), gt_points_all.max())\n",
    "\n",
    "n_iter_points = 50\n",
    "\n",
    "for i in range(iters):\n",
    "    idx = torch.randint(0, gt_points_all.shape[0], (n_iter_points,  ))\n",
    "    selected_points = torch.gather(gt_points_all, 0, idx.unsqueeze(-1).expand(-1, 3))\n",
    "\n",
    "    # Zero gradients at the start of each iteration\n",
    "    opt.zero_grad()\n",
    "    points = gt_points\n",
    "    \n",
    "    # Forward pass and compute loss\n",
    "    if normals is not None:\n",
    "        loss, loss_components = loss_fn(model, selected_points, points_range, normals)\n",
    "    else:\n",
    "        loss, loss_components = loss_fn(model, selected_points, points_range)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    opt.step()\n",
    "    \n",
    "    # Print progress\n",
    "    if i %1000 == 0:\n",
    "        # run evaluation \n",
    "        chamfer_dist, v, f = evaluate_reconstruction(model, gt_mesh_path, resolution=64, bounds=(-1.0, 1.0), n_points=10000)\n",
    "\n",
    "        print(f\"Iter {i}/{iters}, Loss: {loss.item():.6f}, \"\n",
    "              f\"Grad: {loss_components['grad_term']:.6f}, \"\n",
    "              f\"DW: {loss_components['double_well']:.6f}, \"\n",
    "              f\"Recon: {loss_components['reconstruction']:.6f}, \"\n",
    "              f\"Norm: {loss_components['normal_constraint']:.6f}\")\n",
    "\n",
    "        print(f\"Chamfer distance: {chamfer_dist:.6f}\")\n",
    "\n",
    "        # create mesh with marching cubes\n",
    "        write_mesh(v,f,f'intermediates/mesh_{i}.ply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8df06336734f3d5aefcb8cba5f6eb7ddbe787a24176311df2982a9f74ff87f0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('neurecon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
