{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from skimage import measure\n",
    "import open3d as o3d\n",
    "# import your libraries\n",
    "\n",
    "from IGR.code.model.network import ImplicitNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pointcloud(filename):\n",
    "    pcd = o3d.io.read_point_cloud(filename)\n",
    "    points = torch.tensor(np.asarray(pcd.points), dtype=torch.float32)\n",
    "    return points\n",
    "    \n",
    "def write_mesh(v,f,filename):\n",
    "    mesh = o3d.geometry.TriangleMesh(o3d.utility.Vector3dVector(v),o3d.utility.Vector3iVector(f))\n",
    "    o3d.io.write_triangle_mesh(filename,mesh)\n",
    "    \n",
    "def write_pointcloud(p,filename):\n",
    "    pc = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(p))\n",
    "    o3d.io.write_point_cloud(filename,pc)\n",
    "\n",
    "# class ImplcitNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         pass\n",
    "\n",
    "#     def phase_loss(self, x):\n",
    "#         pass\n",
    "\n",
    "class PHASELoss(nn.Module):\n",
    "    def __init__(self, epsilon=0.01, lambda_val=0.1, mu=0.1, ball_radius=0.01, use_normals=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            epsilon: Regularization parameter that controls smoothness\n",
    "            lambda_val: Weight for the reconstruction loss\n",
    "            mu: Weight for the normal/gradient constraint loss\n",
    "            ball_radius: Radius of balls around point samples for reconstruction loss\n",
    "            use_normals: If True, uses provided normals; otherwise enforces unit gradients\n",
    "        \"\"\"\n",
    "        super(PHASELoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.lambda_val = lambda_val\n",
    "        self.mu = mu\n",
    "        self.ball_radius = ball_radius\n",
    "        self.use_normals = use_normals\n",
    "        \n",
    "    def double_well_potential(self, x):\n",
    "        return x**2 - 2*torch.abs(x) + 1\n",
    "    \n",
    "    def reconstruction_loss(self, u, points, sample_count=50):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            u: Neural network representing the signed density\n",
    "            points: Input point cloud (B x N x 3)\n",
    "            sample_count: Number of points to sample in each ball\n",
    "        \"\"\"\n",
    "        batch_size = points.shape[0]\n",
    "        n_points = points.shape[1]\n",
    "        \n",
    "        u_values = []\n",
    "\n",
    "        # Sample points from random subset of input points\n",
    "        for i in range(sample_count):\n",
    "            idx = torch.randint(0, n_points, (batch_size,  ))\n",
    "            selected_points = torch.gather(points, 1, idx.unsqueeze(-1).expand(-1, -1, 3))\n",
    "            \n",
    "            # Generate random offsets within ball_radius\n",
    "            random_offsets = torch.randn_like(selected_points)\n",
    "            random_offsets = random_offsets / torch.norm(random_offsets, dim=-1, keepdim=True)\n",
    "            random_offsets = random_offsets * self.ball_radius * torch.rand_like(random_offsets[..., :1])\n",
    "            \n",
    "            # Sample points within balls\n",
    "            sampled_points = selected_points + random_offsets\n",
    "            \n",
    "            # Evaluate network at sampled points\n",
    "\n",
    "            print(u(sampled_points).shape)\n",
    "            dsalmk\n",
    "\n",
    "            u_values.append(u(sampled_points))\n",
    "        \n",
    "        # Compute average value in each ball\n",
    "        return torch.mean(torch.abs(torch.stack(u_values, dim = 0)))\n",
    "    \n",
    "    def gradient_loss(self, u, w, points, normals=None):\n",
    "        \"\"\"\n",
    "        Computes the gradient constraint loss:\n",
    "        - If normals are provided, aligns gradients with normals\n",
    "        - Otherwise, enforces unit gradient norm\n",
    "        \n",
    "        Args:\n",
    "            u: Neural network representing the signed density\n",
    "            w: Log-transformed function (-sqrt(epsilon) * log(1-|u|) * sign(u))\n",
    "            points: Input point cloud\n",
    "            normals: Surface normals (optional)\n",
    "        \"\"\"\n",
    "        points.requires_grad_(True)\n",
    "        w_val = w(points)\n",
    "        \n",
    "        # Compute gradients of w with respect to input points\n",
    "        grad_outputs = torch.ones_like(w_val)\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=w_val,\n",
    "            inputs=points,\n",
    "            grad_outputs=grad_outputs,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        \n",
    "        if self.use_normals and normals is not None:\n",
    "            # Align gradients with provided normals\n",
    "            return F.l1_loss(gradients, normals)\n",
    "        else:\n",
    "            # Enforce unit gradient norm\n",
    "            gradient_norm = torch.norm(gradients, dim=-1)\n",
    "            return torch.mean((gradient_norm - 1.0)**2)\n",
    "    \n",
    "    def forward(self, model, points, normals=None):\n",
    "        \"\"\"\n",
    "        Computes the complete PHASE loss\n",
    "        \n",
    "        Args:\n",
    "            model: Neural network model for signed density function\n",
    "            points: Input point cloud\n",
    "            normals: Surface normals (optional)\n",
    "        \"\"\"\n",
    "        # Get the signed density function values\n",
    "        u = lambda x: model(x)\n",
    "        \n",
    "        # Define the log-transformed function w (the smoothed SDF)\n",
    "        w = lambda x: -torch.sqrt(self.epsilon) * torch.log(1 - torch.abs(u(x))) * torch.sign(u(x))\n",
    "        \n",
    "        # Sample random points in the domain for regularization term\n",
    "        batch_size = points.shape[0]\n",
    "        domain_points = torch.rand((batch_size, 1000, 3), device=points.device) * 2 - 1\n",
    "        \n",
    "        # Evaluate model on random domain points\n",
    "        u_domain = u(domain_points)\n",
    "        \n",
    "        # Calculate the gradient of u\n",
    "        domain_points.requires_grad_(True)\n",
    "        grad_outputs = torch.ones_like(u_domain)\n",
    "        grad_u = torch.autograd.grad(\n",
    "            outputs=u_domain, \n",
    "            inputs=domain_points,\n",
    "            grad_outputs=grad_outputs,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        \n",
    "        # Double-well potential term\n",
    "        double_well_term = torch.mean(self.double_well_potential(u_domain))\n",
    "        \n",
    "        # Gradient regularization term\n",
    "        grad_term = torch.mean(self.epsilon * torch.sum(grad_u**2, dim=-1))\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        recon_loss = self.reconstruction_loss(u, points)\n",
    "        \n",
    "        # Normal/gradient constraint loss\n",
    "        normal_loss = self.gradient_loss(u, w, points, normals)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = grad_term + double_well_term + self.lambda_val * recon_loss + self.mu * normal_loss\n",
    "        \n",
    "        return total_loss, {\n",
    "            'grad_term': grad_term.item(),\n",
    "            'double_well': double_well_term.item(),\n",
    "            'reconstruction': recon_loss.item(),\n",
    "            'normal_constraint': normal_loss.item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load point cloud\n",
    "points = load_pointcloud('bunny.ply')\n",
    "\n",
    "# instantiate the model and optimizer\n",
    "model = ImplicitNet(d_in = 3, dims = [ 512, 512, 512, 512, 512, 512, 512, 512 ], skip_in = [4], geometric_init = True)\n",
    "opt = torch.optim.Adam(\n",
    "            [\n",
    "                {\n",
    "                    \"params\": model.parameters(),\n",
    "                    \"lr\": 0.005,\n",
    "                    \"weight_decay\": 0\n",
    "                },\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chamfer_distance(pred_points, gt_points):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pred_points (torch.Tensor): Predicted point cloud (N x 3)\n",
    "        gt_points (torch.Tensor): Ground truth point cloud (M x 3)\n",
    "    \"\"\"\n",
    "    # Ensure inputs are on the same device\n",
    "    device = pred_points.device\n",
    "    \n",
    "    # Compute all pairwise distances\n",
    "    pred_expanded = pred_points.unsqueeze(1)  # (N, 1, 3)\n",
    "    gt_expanded = gt_points.unsqueeze(0)      # (1, M, 3)\n",
    "    \n",
    "    # Compute squared distances\n",
    "    dist_matrix = torch.sum((pred_expanded - gt_expanded) ** 2, dim=-1)  # (N, M)\n",
    "    \n",
    "    # Compute minimum distances in both directions\n",
    "    dist_pred_to_gt = torch.min(dist_matrix, dim=1)[0]  # (N,)\n",
    "    dist_gt_to_pred = torch.min(dist_matrix, dim=0)[0]  # (M,)\n",
    "    \n",
    "    # Average the distances (symmetric Chamfer distance)\n",
    "    chamfer_dist = torch.mean(dist_pred_to_gt) + torch.mean(dist_gt_to_pred)\n",
    "    \n",
    "    return chamfer_dist.item()\n",
    "\n",
    "def sample_mesh_points(mesh_path, n_points=10000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        mesh_path (str): Path to the mesh file\n",
    "        n_points (int): Number of points to sample    \n",
    "    \"\"\"\n",
    "    mesh = o3d.io.read_triangle_mesh(mesh_path)\n",
    "    pcd = mesh.sample_points_uniformly(number_of_points=n_points)\n",
    "    points = torch.tensor(np.asarray(pcd.points), dtype=torch.float32)\n",
    "    return points\n",
    "\n",
    "def evaluate_reconstruction(model, gt_mesh_path, resolution=64, bounds=(-1.0, 1.0), n_points=10000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model: Neural network model for implicit function\n",
    "        gt_mesh_path (str): Path to ground truth mesh\n",
    "        resolution (int): Resolution for marching cubes grid\n",
    "        bounds (tuple): Min and max bounds for the grid\n",
    "        n_points (int): Number of points to sample for Chamfer distance    \n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Create grid for marching cubes\n",
    "        x = np.linspace(bounds[0], bounds[1], resolution)\n",
    "        y = np.linspace(bounds[0], bounds[1], resolution)\n",
    "        z = np.linspace(bounds[0], bounds[1], resolution)\n",
    "        \n",
    "        X, Y, Z = np.meshgrid(x, y, z, indexing='ij')\n",
    "        points = torch.tensor(np.stack([X.flatten(), Y.flatten(), Z.flatten()], axis=1), \n",
    "                              dtype=torch.float32)\n",
    "        \n",
    "        # Process in batches to avoid memory issues\n",
    "        batch_size = 10000\n",
    "        sdf_grid = []\n",
    "        for i in range(0, points.shape[0], batch_size):\n",
    "            batch_points = points[i:i+batch_size]\n",
    "            sdf_batch = model(batch_points).detach().cpu().numpy()\n",
    "            sdf_grid.append(sdf_batch)\n",
    "        \n",
    "        sdf_grid = np.concatenate(sdf_grid, axis=0).reshape(resolution, resolution, resolution)\n",
    "        \n",
    "        # Generate mesh using marching cubes\n",
    "        v, f, _, _ = measure.marching_cubes_lewiner(sdf_grid, 0, gradient_direction='ascent')\n",
    "        \n",
    "        # Scale vertices back to original coordinate system\n",
    "        v = v / (resolution - 1) * (bounds[1] - bounds[0]) + bounds[0]\n",
    "        \n",
    "        # First save reconstructed mesh to temporary file\n",
    "        temp_mesh_path = 'temp_reconstruction.ply'\n",
    "        write_mesh(v, f, temp_mesh_path)\n",
    "        \n",
    "        # Sample points from both meshes\n",
    "        pred_points = sample_mesh_points(temp_mesh_path, n_points)\n",
    "        gt_points = sample_mesh_points(gt_mesh_path, n_points)\n",
    "        \n",
    "        # Compute Chamfer distance\n",
    "        chamfer_dist = compute_chamfer_distance(pred_points, gt_points)\n",
    "        \n",
    "    return chamfer_dist, v, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters=100000\n",
    "lam, eps, mu = [10, 0.01, 10]\n",
    "loss_fn = PHASELoss(epsilon=eps, lambda_val=lam, mu=mu, ball_radius=0.001, use_normals=False)\n",
    "\n",
    "gt_mesh_path = \"Preimage_Implicit_DLTaskData/armadillo_10000.xyz\"\n",
    "\n",
    "normals = False\n",
    "\n",
    "if normals:\n",
    "    # Load normals if available\n",
    "    normals = load_pointcloud('bunny_normals.ply')\n",
    "else:    \n",
    "    normals = None\n",
    "\n",
    "model.train()\n",
    "\n",
    "for i in range(iters):\n",
    "    # compute loss and train network\n",
    "    gt_points = sample_mesh_points(gt_mesh_path, n_points=10000)\n",
    "\n",
    "    # Zero gradients at the start of each iteration\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    # Get a batch of points for training\n",
    "    # For simplicity, we use the entire point cloud, but you could implement batch sampling here\n",
    "    points_batch = gt_points.unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    # Forward pass and compute loss\n",
    "    if normals is not None:\n",
    "        loss, loss_components = loss_fn(model, points_batch, normals)\n",
    "    else:\n",
    "        loss, loss_components = loss_fn(model, points_batch)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    opt.step()\n",
    "    \n",
    "    # Print progress\n",
    "    if i %1000 == 0:\n",
    "        # run evaluation \n",
    "        chamfer_dist, v, f = evaluate_reconstruction(model, gt_mesh_path, resolution=64, bounds=(-1.0, 1.0), n_points=10000)\n",
    "\n",
    "        print(f\"Iter {i}/{iters}, Loss: {loss.item():.6f}, \"\n",
    "              f\"Grad: {loss_components['grad_term']:.6f}, \"\n",
    "              f\"DW: {loss_components['double_well']:.6f}, \"\n",
    "              f\"Recon: {loss_components['reconstruction']:.6f}, \"\n",
    "              f\"Norm: {loss_components['normal_constraint']:.6f}\")\n",
    "\n",
    "        print(f\"Chamfer distance: {chamfer_dist:.6f}\")\n",
    "\n",
    "        # create mesh with marching cubes\n",
    "        write_mesh(v,f,f'intermediates/mesh_{i}.ply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8df06336734f3d5aefcb8cba5f6eb7ddbe787a24176311df2982a9f74ff87f0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('neurecon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
